#!/usr/bin/env python3
"""
Automated Publications Updater for Heidar-Zadeh Group Website

This script automatically fetches publication data from Google Scholar
and updates the papers.bib file for the Jekyll website.

Usage:
    python update_publications.py

Requirements:
    pip install requests beautifulsoup4 selenium

Author: Heidar-Zadeh Group Website Automation
"""

import os
import sys
import json
import time
import re
from datetime import datetime
from urllib.parse import quote, unquote

try:
    import requests
    from bs4 import BeautifulSoup
    import json
except ImportError:
    print("Required packages not installed. Please run:")
    print("pip install requests beautifulsoup4")
    sys.exit(1)

# Configuration
SCHOLAR_ID = "JlIWcccAAAAJ"  # Prof. Farnaz Heidar-Zadeh's Google Scholar ID
BIB_FILE_PATH = "_bibliography/papers.bib"
BACKUP_FILE_PATH = "_bibliography/papers.bib.backup"

def backup_existing_bib():
    """Create a backup of the existing papers.bib file"""
    if os.path.exists(BIB_FILE_PATH):
        import shutil
        shutil.copy2(BIB_FILE_PATH, BACKUP_FILE_PATH)
        print(f"‚úì Backup created: {BACKUP_FILE_PATH}")

def get_scholar_publications():
    """Fetch publications from Google Scholar using direct scraping"""
    print(f"üîç Fetching publications for Scholar ID: {SCHOLAR_ID}")
    
    # Set up session with headers to mimic a real browser
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    })
    
    try:
        # Construct the Google Scholar URL
        base_url = f"https://scholar.google.com/citations?user={SCHOLAR_ID}&hl=en&cstart=0&pagesize=100"
        
        print(f"üì° Requesting: {base_url}")
        
        # Add delay to be respectful
        time.sleep(2)
        
        response = session.get(base_url, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Find author name
        author_name = "Unknown Author"
        name_element = soup.find('div', {'id': 'gsc_prf_in'})
        if name_element:
            author_name = name_element.get_text().strip()
            print(f"‚úì Found author: {author_name}")
        
        # Find publication entries
        publications = []
        pub_table = soup.find('table', {'id': 'gsc_a_t'})
        
        if not pub_table:
            print("‚ùå Could not find publications table")
            return []
        
        rows = pub_table.find_all('tr', class_='gsc_a_tr')
        print(f"‚úì Found {len(rows)} publications")
        
        for i, row in enumerate(rows):
            try:
                pub_data = parse_publication_row(row, session)
                if pub_data:
                    publications.append(pub_data)
                    print(f"  üìÑ {i+1}/{len(rows)}: {pub_data.get('title', 'Unknown title')}")
                
                # Add delay between requests
                time.sleep(1)
                
            except Exception as e:
                print(f"  ‚ö†Ô∏è  Error parsing publication {i+1}: {e}")
                continue
        
        # Sort publications by year (newest first), then by citation count (highest first)
        publications.sort(key=lambda x: (x.get('year', 0), x.get('citations', 0)), reverse=True)
        
        return publications
    
    except requests.RequestException as e:
        print(f"‚ùå Network error fetching from Google Scholar: {e}")
        return []
    except Exception as e:
        print(f"‚ùå Error fetching from Google Scholar: {e}")
        return []

def parse_publication_row(row, session):
    """Parse a single publication row from Google Scholar"""
    pub_data = {}
    
    # Get title and link
    title_cell = row.find('td', class_='gsc_a_t')
    if title_cell:
        title_link = title_cell.find('a', class_='gsc_a_at')
        if title_link:
            pub_data['title'] = title_link.get_text().strip()
            pub_data['scholar_url'] = "https://scholar.google.com" + title_link.get('href', '')
        
        # Get authors and venue info
        author_info = title_cell.find('div', class_='gs_gray')
        if author_info:
            author_text = author_info.get_text().strip()
            pub_data['authors_raw'] = author_text
            
            # Try to separate authors from venue
            parts = author_text.split(' - ')
            if len(parts) >= 2:
                pub_data['authors'] = parts[0].strip()
                pub_data['venue'] = parts[1].strip()
            else:
                pub_data['authors'] = author_text
    
    # Get citation count
    citation_cell = row.find('td', class_='gsc_a_c')
    if citation_cell:
        citation_link = citation_cell.find('a', class_='gsc_a_ac')
        if citation_link:
            citation_text = citation_link.get_text().strip()
            if citation_text.isdigit():
                pub_data['citations'] = int(citation_text)
    
    # Get year
    year_cell = row.find('td', class_='gsc_a_y')
    if year_cell:
        year_span = year_cell.find('span', class_='gsc_a_h')
        if year_span:
            year_text = year_span.get_text().strip()
            if year_text.isdigit():
                pub_data['year'] = int(year_text)
    
    return pub_data if pub_data.get('title') else None

def format_author_name(author_name):
    """Format author name for BibTeX"""
    # Handle special characters and formatting
    name = author_name.replace("*", "").replace("‚Ä†", "").strip()
    
    # Split name into parts
    parts = name.split()
    if len(parts) >= 2:
        last_name = parts[-1]
        first_names = " ".join(parts[:-1])
        return f"{last_name}, {first_names}"
    return name

def generate_bibtex_key(title, year):
    """Generate a unique BibTeX key"""
    # Clean title and take first few words
    clean_title = ''.join(c for c in title if c.isalnum() or c.isspace())
    words = clean_title.split()[:3]
    key_base = ''.join(words).lower()
    return f"heidarzadeh{year}{key_base}"

def publication_to_bibtex(pub):
    """Convert a scraped publication to BibTeX format"""
    
    # Extract basic information
    title = pub.get('title', 'Unknown Title')
    year = pub.get('year', datetime.now().year)
    authors_raw = pub.get('authors', '')
    venue = pub.get('venue', '')
    
    # Clean and format title
    title = clean_bibtex_string(title)
    
    # Generate BibTeX key
    bib_key = generate_bibtex_key(title, year)
    
    # Format authors
    authors_str = format_authors_for_bibtex(authors_raw)
    
    # Determine publication type and venue
    pub_type = "article"  # Default
    journal = ""
    booktitle = ""
    
    if venue:
        venue = clean_bibtex_string(venue)
        # Simple heuristic to determine if it's a journal or conference
        conference_keywords = ['conference', 'proceedings', 'symposium', 'workshop', 'meeting', 'proc']
        if any(keyword in venue.lower() for keyword in conference_keywords):
            pub_type = "inproceedings"
            booktitle = venue
        else:
            journal = venue
    
    # Build BibTeX entry
    bibtex_lines = [f"@{pub_type}{{{bib_key},"]
    bibtex_lines.append(f"  bibtex_show={{true}},")
    bibtex_lines.append(f"  title={{{title}}},")
    
    if authors_str:
        bibtex_lines.append(f"  author={{{authors_str}}},")
    
    if journal:
        bibtex_lines.append(f"  journal={{{journal}}},")
    if booktitle:
        bibtex_lines.append(f"  booktitle={{{booktitle}}},")
    
    bibtex_lines.append(f"  year={{{year}}},")
    
    # Add citation count if available
    if 'citations' in pub and pub['citations']:
        bibtex_lines.append(f"  note={{Cited by {pub['citations']}}},")
    
    # Add Google Scholar URL if available
    if 'scholar_url' in pub:
        bibtex_lines.append(f"  url={{{pub['scholar_url']}}},")
    
    bibtex_lines.append("}")
    
    return "\n".join(bibtex_lines)

def clean_bibtex_string(text):
    """Clean a string for use in BibTeX"""
    if not text:
        return ""
    
    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text.strip())
    
    # Handle special characters that might break BibTeX
    # Keep basic punctuation but escape problematic characters
    text = text.replace('{', '').replace('}', '')
    
    return text

def format_authors_for_bibtex(authors_raw):
    """Format author string for BibTeX"""
    if not authors_raw:
        return ""
    
    # Clean the authors string
    authors_raw = clean_bibtex_string(authors_raw)
    
    # Split by common separators
    if ',' in authors_raw:
        # If there are commas, split by them
        authors = [author.strip() for author in authors_raw.split(',')]
    else:
        # Otherwise, try to split by 'and'
        authors = [author.strip() for author in re.split(r'\s+and\s+', authors_raw, flags=re.IGNORECASE)]
    
    # Format each author
    formatted_authors = []
    for author in authors:
        if author:
            formatted_author = format_author_name(author)
            if formatted_author:
                formatted_authors.append(formatted_author)
    
    return " and ".join(formatted_authors)

def update_papers_bib(publications):
    """Update the papers.bib file with new publications"""
    
    print(f"üìù Updating {BIB_FILE_PATH}")
    
    # Create BibTeX header
    header = f"""---
---

@comment{{
  This file is automatically generated from Google Scholar.
  Last updated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
  Scholar ID: {SCHOLAR_ID}
  
  DO NOT EDIT MANUALLY - Changes will be overwritten on next update.
  To update: run python _scripts/update_publications.py
}}

"""
    
    # Generate BibTeX entries
    bibtex_entries = []
    for pub in publications:
        try:
            entry = publication_to_bibtex(pub)
            bibtex_entries.append(entry)
        except Exception as e:
            print(f"  ‚ö†Ô∏è  Error converting publication to BibTeX: {e}")
            continue
    
    # Write to file
    with open(BIB_FILE_PATH, 'w', encoding='utf-8') as f:
        f.write(header)
        f.write("\n\n".join(bibtex_entries))
    
    print(f"‚úì Updated {BIB_FILE_PATH} with {len(bibtex_entries)} publications")

def main():
    """Main function"""
    print("üöÄ Starting automated publications update...")
    print(f"üìÖ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # Create backup
    backup_existing_bib()
    
    # Fetch publications
    publications = get_scholar_publications()
    
    if not publications:
        print("‚ùå No publications found. Keeping existing file.")
        return
    
    # Update BibTeX file
    update_papers_bib(publications)
    
    print()
    print("‚úÖ Publications update completed successfully!")
    print(f"üìä Total publications processed: {len(publications)}")
    print()
    print("Next steps:")
    print("1. Review the updated papers.bib file")
    print("2. Commit changes to your repository")
    print("3. The website will automatically rebuild with new publications")

if __name__ == "__main__":
    main()
